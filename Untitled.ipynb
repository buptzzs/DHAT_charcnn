{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import os\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/zzs/data/qangaroo_v1.1/wikihop/'\n",
    "train_path = 'train.json'\n",
    "dev_path = 'dev.json'\n",
    "\n",
    "dev_json_path = os.path.join(root, dev_path)\n",
    "dev_ex_path = dev_json_path.replace('.json','_test.pt')\n",
    "dev_data = json.load(open(dev_json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data as textdata, vocab\n",
    "from torchtext.data import Field\n",
    "import copy\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "class CharField(Field):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pad_token='<pad>',\n",
    "        unk_token='<unk>',\n",
    "        batch_first=True,\n",
    "        max_word_length=20,\n",
    "        max_sentence_length=128,\n",
    "        lower=True,\n",
    "        **kwargs):\n",
    "        super().__init__(\n",
    "            sequential=True,  # Otherwise pad is set to None in textdata.Field\n",
    "            batch_first=batch_first,\n",
    "            use_vocab=True,\n",
    "            pad_token=pad_token,\n",
    "            unk_token=unk_token,\n",
    "            lower=lower,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.max_word_length = max_word_length\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "    def build_vocab(self, *args, **kwargs):\n",
    "        sources = []\n",
    "        for arg in args:\n",
    "            if isinstance(arg, textdata.Dataset):\n",
    "                sources += [\n",
    "                    getattr(arg, name)\n",
    "                    for name, field in arg.fields.items()\n",
    "                    if field is self\n",
    "                ]\n",
    "            else:\n",
    "                sources.append(arg)\n",
    "\n",
    "        counter = Counter()\n",
    "        for data in sources:\n",
    "            # data is the return value of preprocess().\n",
    "            for para in data:\n",
    "                if isinstance(para[0], list):\n",
    "                    for sentence in para:\n",
    "                        for word_chars in sentence:\n",
    "                            counter.update(word_chars)\n",
    "                else:\n",
    "                    for word_chars in para:\n",
    "                        counter.update(word_chars)                    \n",
    "       \n",
    "        specials = [self.unk_token, self.pad_token]\n",
    "\n",
    "        self.vocab = vocab.Vocab(counter, specials=specials, **kwargs)\n",
    "        \n",
    "    def pad(self, minibatch: List[List[List[str]]]) -> List[List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Example of minibatch:\n",
    "        ::\n",
    "            [[['p', 'l', 'a', 'y', '<PAD>', '<PAD>'],\n",
    "              ['t', 'h', 'a', 't', '<PAD>', '<PAD>'],\n",
    "              ['t', 'r', 'a', 'c', 'k', '<PAD>'],\n",
    "              ['o', 'n', '<PAD>', '<PAD>', '<PAD>', '<PAD>'],\n",
    "              ['r', 'e', 'p', 'e', 'a', 't']\n",
    "             ], ...\n",
    "            ]\n",
    "        \"\"\"\n",
    "        # If we change the same minibatch object then the underlying data\n",
    "        # will get corrupted. Hence deep copy the minibatch object.\n",
    "        \n",
    "        if self.max_sentence_length is not None:\n",
    "            max_sentence_length = self.max_sentence_length\n",
    "        else:\n",
    "            max_sentence_length = max(len(sent) for sent in minibatch)\n",
    "\n",
    "        if self.max_word_length is not None:\n",
    "            max_word_length = self.max_word_length            \n",
    "        else:\n",
    "            max_word_length = max(len(word) for sent in minibatch for word in sent)        \n",
    "\n",
    "        padded_minibatch = []\n",
    "        for sentence in minibatch:\n",
    "            sentence_ch = []\n",
    "            for word in sentence[:max_sentence_length]:\n",
    "                sentence_ch.append(list(word))\n",
    "            padded_minibatch.append(sentence_ch)\n",
    "\n",
    "        for i, sentence in enumerate(padded_minibatch):\n",
    "            for j, word in enumerate(sentence):\n",
    "                char_padding = [self.pad_token] * (max_word_length - len(word))\n",
    "                padded_minibatch[i][j].extend(char_padding)\n",
    "                padded_minibatch[i][j] = padded_minibatch[i][j][:max_word_length]\n",
    "            if len(sentence) < max_sentence_length:\n",
    "                for _ in range(max_sentence_length - len(sentence)):\n",
    "                    char_padding = [self.pad_token] * max_word_length\n",
    "                    padded_minibatch[i].append(char_padding)\n",
    "\n",
    "        return padded_minibatch\n",
    "\n",
    "    def numericalize(self, batch, device=None):\n",
    "        batch_char_ids = []\n",
    "        for sentence in batch:\n",
    "            sentence_char_ids = super().numericalize(sentence, device=device)\n",
    "            batch_char_ids.append(sentence_char_ids)\n",
    "        return torch.stack(batch_char_ids, dim=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset,Iterator, RawField, BucketIterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab, FastText\n",
    "\n",
    "\n",
    "class DocCharField(CharField):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        memory_size=None,\n",
    "        max_word_length=20,\n",
    "        max_sentence_length=128,\n",
    "        keep_sent_len=128,\n",
    "        keep_word_len=10,\n",
    "        **kwargs\n",
    "        ):\n",
    "        tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        super(DocCharField, self).__init__(tokenize=tokenizer, **kwargs)\n",
    "        self.memory_size = memory_size\n",
    "        self.keep_sent_len = keep_sent_len\n",
    "        self.keep_word_len = keep_word_len\n",
    "\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        if isinstance(x, list):\n",
    "            ss =  super(DocCharField, self).preprocess(x)\n",
    "            return [super(DocCharField, self).preprocess(s) for s in ss]\n",
    "        else:\n",
    "            return super(DocCharField, self).preprocess(x)\n",
    "\n",
    "    def pad(self, minibatch):\n",
    "        if isinstance(minibatch[0][0], list):\n",
    "            self.max_sentence_length = max(max(len(x) for x in ex) for ex in minibatch)\n",
    "            if self.keep_sent_len is not None:\n",
    "                self.max_sentence_length = min(self.keep_sent_len, self.max_sentence_length)\n",
    "            self.max_word_length = max([len(word) for para in minibatch for sent in para for word in sent ])\n",
    "            if self.keep_word_len is not None:\n",
    "                self.max_word_length = min(self.keep_word_len, self.max_word_length)\n",
    "                \n",
    "            if self.memory_size is None:\n",
    "                memory_size = max(len(ex) for ex in minibatch)\n",
    "            else:\n",
    "                memory_size = self.memory_size\n",
    "            padded = []\n",
    "            for ex in minibatch:\n",
    "                # sentences are indexed in reverse order and truncated to memory_size\n",
    "                nex = ex[:memory_size]\n",
    "                padded.append(\n",
    "                    super(DocCharField, self).pad(nex)\n",
    "                )\n",
    "                for _ in range(memory_size - len(nex)):\n",
    "                    padded_sent = [[self.pad_token]*self.max_word_length for _ in range(self.max_sentence_length)]\n",
    "                    padded[-1].append(padded_sent)\n",
    "            return padded\n",
    "        else:\n",
    "            self.max_sentence_length = None\n",
    "            self.max_word_length = max([len(word) for sent in minibatch for word in sent ])\n",
    "            if self.keep_word_len is not None:\n",
    "                self.max_word_length = min(self.keep_word_len, self.max_word_length)\n",
    "            return super(DocCharField, self).pad(minibatch)\n",
    "\n",
    "    def numericalize(self, arr, device=None):\n",
    "        if isinstance(arr[0][0][0], list):\n",
    "            tmp = [\n",
    "                super(DocCharField, self).numericalize(x, device=device).data\n",
    "                for x in arr\n",
    "            ]\n",
    "            arr = torch.stack(tmp)\n",
    "            if self.sequential:\n",
    "                arr = arr.contiguous()\n",
    "            return arr\n",
    "        else:\n",
    "            return super(DocCharField, self).numericalize(arr, device=device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_char_field = DocCharField(keep_sent_len=128, keep_word_len=30,lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_char_field = DocCharField(keep_sent_len=256, keep_word_len=16,lower=True)\n",
    "fields = {\n",
    "   'candidates': ('candidates_char',doc_char_field),\n",
    "   'supports': ('supports_char',doc_char_field),\n",
    "    'query': ('query_char', doc_char_field),\n",
    "    'id': ('id', RawField()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "make_example = torchtext.data.example.Example.fromdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5129/5129 [01:40<00:00, 51.20it/s]\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "def preprocess(item):\n",
    "    answer = item['answer']\n",
    "    candidates = item['candidates']\n",
    "    label = candidates.index(answer)\n",
    "    item['label'] = label\n",
    "    item['query'] = item['query'].replace('_',' ')\n",
    "    return item\n",
    "\n",
    "for d in tqdm(dev_data):\n",
    "    d = preprocess(d)\n",
    "    example = make_example(d, fields)\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(fields, dict):\n",
    "    fields, field_dict = [], fields\n",
    "    for field in field_dict.values():\n",
    "        if isinstance(field, list):\n",
    "            fields.extend(field)\n",
    "        else:\n",
    "            fields.append(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_char_field.build_vocab(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_iter = BucketIterator(dataset, 4 ,sort_key=lambda x: len(x.supports_char), sort_within_batch=True, device=None)\n",
    "test_iter = BucketIterator(dataset, 4,device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x46x3x12]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x25x240x16]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x8x9]\n",
      "\t[.id]:['WH_dev_3042', 'WH_dev_2533', 'WH_dev_2341', 'WH_dev_2735']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x48x3x13]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x20x256x16]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x5x10]\n",
      "\t[.id]:['WH_dev_593', 'WH_dev_2066', 'WH_dev_4297', 'WH_dev_4078']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x47x4x15]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x27x236x15]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x15x14]\n",
      "\t[.id]:['WH_dev_4753', 'WH_dev_2939', 'WH_dev_2956', 'WH_dev_3823']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x27x3x12]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x19x256x16]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x6x13]\n",
      "\t[.id]:['WH_dev_115', 'WH_dev_4077', 'WH_dev_2640', 'WH_dev_3857']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x46x4x12]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x29x256x16]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x11x15]\n",
      "\t[.id]:['WH_dev_560', 'WH_dev_4', 'WH_dev_2925', 'WH_dev_2832']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x11x3x10]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x10x217x15]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x6x10]\n",
      "\t[.id]:['WH_dev_700', 'WH_dev_1318', 'WH_dev_551', 'WH_dev_2313']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x55x5x13]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x26x233x15]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x9x14]\n",
      "\t[.id]:['WH_dev_396', 'WH_dev_3716', 'WH_dev_3416', 'WH_dev_4582']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x30x3x13]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x17x256x16]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x7x11]\n",
      "\t[.id]:['WH_dev_3945', 'WH_dev_1649', 'WH_dev_87', 'WH_dev_223']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x17x3x13]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x25x256x16]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x5x9]\n",
      "\t[.id]:['WH_dev_3807', 'WH_dev_4063', 'WH_dev_3114', 'WH_dev_1763']\n",
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.candidates_char]:[torch.LongTensor of size 4x38x4x11]\n",
      "\t[.supports_char]:[torch.LongTensor of size 4x17x222x15]\n",
      "\t[.query_char]:[torch.LongTensor of size 4x5x8]\n",
      "\t[.id]:['WH_dev_3095', 'WH_dev_1217', 'WH_dev_2206', 'WH_dev_218']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in test_iter:\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_prefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_data = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            return\n",
    "            \n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        data = self.next_data\n",
    "        self.preload()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefetcher = data_prefetcher(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "batch = prefetcher.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3,  6,  6,  ...,  1,  1,  1],\n",
       "          [ 3,  9, 20,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[20,  7,  8,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[20,  9,  3,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8,  3, 19,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 8,  4,  9,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[14,  6,  5,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]]],\n",
       "\n",
       "\n",
       "        [[[ 3,  9,  4,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 3, 14,  4,  ..., 17, 10, 19],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[20,  5,  7,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]]],\n",
       "\n",
       "\n",
       "        [[[40,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[20,  2, 19,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[12,  2,  3,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]]],\n",
       "\n",
       "\n",
       "        [[[ 3, 13,  4,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 3, 13,  4,  ...,  1,  1,  1],\n",
       "          [18,  3, 15,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 3,  6,  5,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[22,  5, 11,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]],\n",
       "\n",
       "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "          [ 1,  1,  1,  ...,  1,  1,  1]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.candidates_char"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
